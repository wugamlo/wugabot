I need to integrate our Vector Store Management API into our chatbot application. The API allows us to perform Retrieval-Augmented Generation (RAG) to enhance responses with relevant knowledge from our vector database.
## Requirements:
1. Add a RAG toggle feature in the settings panel (default: OFF)
2. Add a dropdown to select which collection to use for retrieval
3. Implement API calls to retrieve relevant context before generating responses
## API Details:
Base URL: [https://wugamlo-vector-store.replit.app/api]
Endpoints:
- GET /collections - Lists all available collections
- POST /collections/{collection_name}/search - Searches documents within a collection
Search Endpoint Parameters:
- query: The user's question or prompt
- limit: Maximum number of results to return (default: 5)
Search Response Format:
```json
{
  "results": [
    {
      "text": "The document chunk content",
      "score": 0.87,
      "metadata": {
        "source": "document_source",
        "filename": "example.pdf",
        "created_at": "2023-06-15 14:30:22"
      }
    },
    ...
  ]
}
Implementation Steps:
Add a toggle switch in the settings panel labeled "Enable RAG" (default: OFF)
Add a dropdown labeled "Knowledge Base" that populates with available collections
When the toggle is ON and a collection is selected:
a. Before generating a response, send the user's query to the search endpoint
b. Include the retrieved context in the prompt to the LLM
c. Format the context in a way that helps the LLM use it effectively
Sample API Usage:
// Fetch collections for dropdown
async function fetchCollections() {
  const response = await fetch('https://wugamlo-vector-store.replit.app/api/collections');
  const data = await response.json();
  return data.collections;
}
// Search for relevant context
async function searchCollection(collectionName, query, limit = 5) {
  const response = await fetch(`https://wugamlo-vector-store.replit.app/api/collections/${collectionName}/search`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ query, limit })
  });
  const data = await response.json();
  return data.results;
}
Enhanced Prompt Structure:
When RAG is enabled, structure the prompt to the LLM like this:

[System Instructions]
CONTEXT:
{retrieved context from vector store}
USER QUERY:
{user's question}
Please use the context provided above to answer the user's query. If the context doesn't contain relevant information, rely on your general knowledge but acknowledge this fact.
No additional API authentication secrets are needed as the API appears to be publicly accessible. If authorization is required later, we'll need to add appropriate API keys to the Secrets tool.

Please implement this functionality while maintaining the existing UI styling and user experience.